{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC1n7Pe00m4V",
    "outputId": "697a1164-2e2e-4119-c02e-50d8d85eac45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SrqrGO3GlUUI"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TueP-uFSqCGF",
    "outputId": "8a0e0d40-e15b-42cf-ff41-415a51638767"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dabPs2ELn_Fl"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer1(x) + x\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tjSIweV3lD7s"
   },
   "outputs": [],
   "source": [
    "model = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bstbx7mLGBDq"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbFY7l3DAdNQ",
    "outputId": "7b0c2417-1f0a-4f17-f3c8-9fef53f750f6"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "#train_dataset = datasets.EuroSAT(root='data/', transform=transforms.ToTensor(), download=True)\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MO-Z5jl7w7XY"
   },
   "outputs": [],
   "source": [
    "def create_validationset(trainset, batch_size=64):\n",
    "  # Define the indices for the train and validation sets\n",
    "  '''\n",
    "  train_indices = list(range(len(trainset)))\n",
    "  train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices[:45000])\n",
    "  val_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices[45000:])\n",
    "\n",
    "  # Create the train and validation datasets\n",
    "  trainset1 = Subset(trainset, train_indices[:45000])\n",
    "  valset = Subset(trainset, train_indices[45000:])\n",
    "  '''\n",
    "  # Define the indices for training and validation sets\n",
    "  n_train = len(train_dataset)\n",
    "  train_idx = list(range(n_train))\n",
    "  val_size = 0.2\n",
    "  val_count = int(val_size * n_train)\n",
    "  val_idx = torch.randperm(n_train)[:val_count]\n",
    "  train_idx = list(set(train_idx) - set(val_idx))\n",
    "\n",
    "  # Define samplers for the train and validation sets\n",
    "  train_sampler = SubsetRandomSampler(train_idx)\n",
    "  val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "  # Set up the data loaders for the train and validation datasets\n",
    "  trainloader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "  valloader = DataLoader(trainset, batch_size=batch_size, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "  return trainloader,valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ppRqfT6BxSk9"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_validationset(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YWplkBGMxD4A"
   },
   "outputs": [],
   "source": [
    "def split_train(trainset, num_labeled=1000, num_classes=10):\n",
    "\n",
    "  num_per_class = int(num_labeled / num_classes)  # number of labeled data points per class\n",
    "\n",
    "  labeled_indices = []\n",
    "  unlabeled_indices = []\n",
    "\n",
    "  for i in range(num_classes):\n",
    "      indices = np.where(np.array(trainset.targets) == i)[0]\n",
    "      np.random.shuffle(indices)\n",
    "      labeled_indices.extend(indices[:num_per_class])\n",
    "      unlabeled_indices.extend(indices[num_per_class:num_per_class+num_per_class//2])\n",
    "\n",
    "  # Create data loaders for labeled and unlabeled data\n",
    "  labeled_sampler = torch.utils.data.sampler.SubsetRandomSampler(labeled_indices)\n",
    "  unlabeled_sampler = torch.utils.data.sampler.SubsetRandomSampler(unlabeled_indices)\n",
    "\n",
    "  labeled_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=labeled_sampler)\n",
    "  unlabeled_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=unlabeled_sampler)\n",
    "\n",
    "  torch.save({\n",
    "      'labeled_loader': labeled_loader,\n",
    "      'unlabeled_loader': unlabeled_loader\n",
    "  }, \"content/Labeled1/SplittedLoaders.pt\")\n",
    "\n",
    "  return labeled_loader, unlabeled_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QFocj43PxGbG"
   },
   "outputs": [],
   "source": [
    "labeled_loader, unlabeled_loader = split_train(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tOstYcgD0o6X"
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "  if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "    init.xavier_uniform_(m.weight.data)  # initialize weights using Xavier uniform\n",
    "    if m.bias is not None:\n",
    "      init.constant_(m.bias.data, 0.0)  # initialize biases to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7eAoZpdJE4pW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validation(val_loader, model, criterion):\n",
    "  # Evaluate the model on the validation set\n",
    "  with torch.no_grad():\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in val_loader:\n",
    "      inputs, labels = batch\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "      val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return val_loss,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IJBuHdASx01u"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validation_binary(val_loader, model, criterion):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, (inputs, labels) in enumerate(val_loader):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "      val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return val_loss,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y75pZbVuxEdC"
   },
   "outputs": [],
   "source": [
    "def train(trainloader,model, num_epochs,val_loader):\n",
    "  # Define the loss function and optimizer\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "  allEpochs=[]\n",
    "  allLoss=[]\n",
    "  allval = []\n",
    "\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "\n",
    "  # Train the model\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "      # Send the images and labels to the device\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Backward pass and optimization\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Update the running loss\n",
    "      running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss, val_acc = validation(val_loader, model,criterion)\n",
    "\n",
    "    # Print the average training loss for the epoch\n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Additional information\n",
    "    EPOCH = epoch\n",
    "    PATH = \"content/Labeled1/Newfulldatamodel.pt\"\n",
    "    Path_el = \"content/Labeled1/NewfulldatamodelEpochsandLoss.pt\"\n",
    "    LOSS = loss.item()\n",
    "    allEpochs.append(EPOCH)\n",
    "    allLoss.append(LOSS)\n",
    "    allval.append(val_loss)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': EPOCH,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': LOSS,\n",
    "        'val_loss': allval,\n",
    "        'val_acc': val_acc,\n",
    "        'outputs': outputs,\n",
    "        'seed': torch.seed()\n",
    "        }, PATH)\n",
    "\n",
    "    torch.save({\n",
    "          'epoch': allEpochs,\n",
    "          'loss': allLoss,\n",
    "          'val_loss': allval,\n",
    "          'val_acc': val_acc\n",
    "          }, Path_el)\n",
    "\n",
    "\n",
    "  return model,allEpochs,allLoss,allval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lpdvyHcWw0AM"
   },
   "outputs": [],
   "source": [
    "def train_binary(trainloader,model, num_epochs, b, val_loader):\n",
    "  # Define the loss function and optimizer\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "  allEpochs=[]\n",
    "  allLoss=[]\n",
    "  allval = []\n",
    "\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "\n",
    "  # Train the model\n",
    "  for epoch in range(num_epochs):  # replace with the desired number of epochs\n",
    "      running_loss = 0.0\n",
    "      for i, (images, labels) in enumerate(trainloader):\n",
    "\n",
    "        # Send the images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the running loss\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "      val_loss, val_acc = validation_binary(val_loader,model,criterion)\n",
    "\n",
    "      # Print the average training loss for the epoch\n",
    "      epoch_loss = running_loss / len(trainloader.dataset)\n",
    "      print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "      # Additional information\n",
    "      EPOCH = epoch\n",
    "      PATH = \"content/Labeled1/Newbinarydatamodel\" + str(b) + \".pt\"\n",
    "      Path_el = \"content/Labeled1/NewbinarydatamodelEpochsandLoss\" + str(b) + \".pt\"\n",
    "\n",
    "      LOSS = loss.item()\n",
    "      allEpochs.append(EPOCH)\n",
    "      allLoss.append(LOSS)\n",
    "      allval.append(val_loss)\n",
    "\n",
    "      torch.save({\n",
    "          'epoch': EPOCH,\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "          'loss': LOSS,\n",
    "          'val_loss': val_loss,\n",
    "          'val_accuracy': val_acc,\n",
    "          'outputs': outputs,\n",
    "          'class_num': b,\n",
    "          'seed': torch.seed()\n",
    "          }, PATH)\n",
    "\n",
    "      torch.save({\n",
    "          'epoch': allEpochs,\n",
    "          'loss': allLoss,\n",
    "          'val_loss': allval,\n",
    "          'val_accuracy': val_acc,\n",
    "          'class_num': b\n",
    "          }, Path_el)\n",
    "\n",
    "\n",
    "  # Save the trained model\n",
    "  #torch.save(model.state_dict(), 'binary_classifier.pt')\n",
    "  return model, allEpochs, allLoss, allval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QRqsP2v3B2ZO"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "  Path_acc = \"content/Labeled1/Fullaccuracy.pt\"\n",
    "\n",
    "  model.eval()\n",
    "  # Evaluate the model\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    torch.save({\n",
    "          'epoch': accuracy\n",
    "          }, Path_acc)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1Dugt7qqlP9p"
   },
   "outputs": [],
   "source": [
    "def test_binary(model, test_loader,b):\n",
    "  Path_acc = \"content/Labeled1/Binaryaccuracy\" + str(b) + \".pt\"\n",
    "  model.eval()\n",
    "  # Evaluate the model\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "      images = images[0].to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      #print(predicted)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    torch.save({\n",
    "          'epoch': accuracy\n",
    "          }, Path_acc)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_-a1Y7WLCIbo"
   },
   "outputs": [],
   "source": [
    "def load_model_binary():\n",
    "  ch = torch.load(\"content/Labeled1/Newfulldatamodel.pt\", device)\n",
    "  model1 = ResNet18().to(device)\n",
    "  model1.load_state_dict(ch['model_state_dict'])\n",
    "  model1.eval()\n",
    "  in_features = model1.fc.in_features  # get number of input features of last layer\n",
    "  model1.fc = nn.Linear(in_features, 2)\n",
    "\n",
    "  return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lmGWt5WaGlUB"
   },
   "outputs": [],
   "source": [
    "def freeze_layers(model1):\n",
    "  # Freeze first 5 convolutional layers\n",
    "  for param in model1.conv1.parameters():\n",
    "      param.requires_grad = False\n",
    "  for param in model1.layer1.parameters():\n",
    "      param.requires_grad = False\n",
    "  for param in model1.layer2.parameters():\n",
    "      param.requires_grad = False\n",
    "  for param in model1.layer3.parameters():\n",
    "      param.requires_grad = False\n",
    "  for param in model1.layer4.parameters():\n",
    "      param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BM_dNYYgivfB"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XjWZiMqAMqv5"
   },
   "outputs": [],
   "source": [
    "def getbinaryloader(loader, reqlabel):\n",
    "  dataset = loader.dataset\n",
    "  # Initialize lists for positive and negative samples\n",
    "  positive_samples = []\n",
    "  negative_samples = []\n",
    "  positive_samples_targets = []\n",
    "  negative_samples_targets = []\n",
    "  data_pos = []\n",
    "  data_neg = []\n",
    "  print(\"initial\")\n",
    "\n",
    "  # Iterate over the dataset and separate the samples for each class\n",
    "  for i in range(len(dataset)):\n",
    "      sample, target = dataset[i]\n",
    "      if target == reqlabel:\n",
    "          positive_samples.append(i)\n",
    "          data_pos.append(sample)\n",
    "      else:\n",
    "          negative_samples.append(i)\n",
    "          data_neg.append(sample)\n",
    "\n",
    "  print(\"After For\")\n",
    "\n",
    "  num_samples_pos = len(positive_samples)\n",
    "  num_samples_neg = len(negative_samples)\n",
    "\n",
    "  # Create a list of targets for the selected samples\n",
    "  selected_targets = [1]*num_samples_pos + [0]*num_samples_neg\n",
    "  selected_data = data_pos + data_neg\n",
    "\n",
    "  del data_pos\n",
    "  del data_neg\n",
    "\n",
    "  print(\"Creating dataset\")\n",
    "  selected_dataset = MyDataset(selected_data, selected_targets)\n",
    "\n",
    "  # Create a new dataloader with the selected and shuffled samples\n",
    "  batch_size = 64\n",
    "  dataloader = data.DataLoader(selected_dataset, batch_size=batch_size, shuffle=True)\n",
    "  del selected_dataset\n",
    "\n",
    "  return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hre_UHly1wGH"
   },
   "outputs": [],
   "source": [
    "def train_full(train_loader, val_loader, num_epochs):\n",
    "  model = ResNet18().to(device)\n",
    "  # Define the loss function and optimizer\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "  #scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "  model.apply(weight_init)\n",
    "  print(\"Training\")\n",
    "  model_tr,epoc,trloss,valloss = train(train_loader, model, num_epochs, val_loader)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "S72OD_ah16Rv"
   },
   "outputs": [],
   "source": [
    "def train_binary_call(num, train_loader,vl,nepochs):\n",
    "  dl0 = getbinaryloader(train_loader,num)\n",
    "  val_loader1 = getbinaryloader(vl,num)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  model1 = load_model_binary().to(device)\n",
    "  optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "  #scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "  model1.apply(weight_init)\n",
    "  freeze_layers(model1)\n",
    "  md_trained,epoc,trloss,valloss = train_binary(dl0, model1,nepochs,num,val_loader1)\n",
    "\n",
    "  return md_trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3FaUgqqdaL3q"
   },
   "outputs": [],
   "source": [
    "def predict_pseudolabels(testloader, model):\n",
    "\n",
    "  # Evaluate the model on the test set\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  prob = []\n",
    "  indices_to_remove = []\n",
    "  data = testloader.dataset\n",
    "  print(\"loader: \", len(data))\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "      # Forward pass and compute loss\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      outputs = torch.sigmoid(outputs)\n",
    "\n",
    "      a,predicted = torch.max(outputs.data, dim=1)\n",
    "\n",
    "      for i in range(len(predicted)):\n",
    "        if a[i] >= 0.9:\n",
    "          indices_to_remove.append(batch_idx * testloader.batch_size + i)\n",
    "          prob.append(a[i])\n",
    "  print(\"Indices: \", len(indices_to_remove))\n",
    "      # Remove data from the dataset\n",
    "  # create a subset of the dataset with some data removed\n",
    "  after_remove_unlabeled = Subset(data, [i for i in range(len(data)) if i not in indices_to_remove])\n",
    "  labeled_to_train = Subset(data, indices_to_remove)\n",
    "\n",
    "  return labeled_to_train, after_remove_unlabeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "cUKNoF3UaUx1"
   },
   "outputs": [],
   "source": [
    "def getdataloaders(labeled_data, unlabeled_data, new_dataset,i):\n",
    "\n",
    "  # append the new dataset to the existing dataset\n",
    "  new_dataset.targets = [i]*len(new_dataset)\n",
    "  full_dataset = ConcatDataset([labeled_data, new_dataset])\n",
    "\n",
    "  # Create a new dataloader with the selected and shuffled samples\n",
    "  batch_size = 64\n",
    "  #indices = list(range(len(full_dataset)))\n",
    "  #sampler = SubsetRandomSampler(indices)\n",
    "  dataloader_full = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_unlabeled = DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  del full_dataset\n",
    "\n",
    "  return dataloader_full,dataloader_unlabeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Bbej05LwjGb2"
   },
   "outputs": [],
   "source": [
    "def sortmodels():\n",
    "  # Define a list to store the models and their validation accuracy\n",
    "  models = []\n",
    "\n",
    "  # Loop over the models and compute validation accuracy for each one\n",
    "  for i in range(10):\n",
    "      ch = torch.load(\"content/Labeled1/Newbinarydatamodel9\" + \".pt\",device)\n",
    "      model1 = ResNet18(num_classes=2).to(device)\n",
    "      model1.load_state_dict(ch[\"model_state_dict\"])\n",
    "      model1.eval()\n",
    "      print(ch.keys())\n",
    "      acc = ch[\"val_accuracy\"]\n",
    "      models.append({'model': model1, 'accuracy': acc, 'classifier': i})\n",
    "\n",
    "  # Sort the models based on validation accuracy in descending order\n",
    "  models.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "  # Print the sorted list of models and their validation accuracy\n",
    "  for model in models:\n",
    "      print(f\"Model: {model['model']} Validation Accuracy: {model['accuracy']} classifier: {model['classifier']}\")\n",
    "\n",
    "  return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "f4L41oV8vVcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Validation Accuracy: 56.02%\n",
      "Epoch 1/100, Training Loss: 0.0349, Validation Loss: 0.2518\n",
      "Validation Accuracy: 80.31%\n",
      "Epoch 2/100, Training Loss: 0.0136, Validation Loss: 0.1319\n",
      "Validation Accuracy: 87.53%\n",
      "Epoch 3/100, Training Loss: 0.0062, Validation Loss: 0.0846\n",
      "Validation Accuracy: 91.86%\n",
      "Epoch 4/100, Training Loss: 0.0038, Validation Loss: 0.0557\n",
      "Validation Accuracy: 91.63%\n",
      "Epoch 5/100, Training Loss: 0.0021, Validation Loss: 0.0557\n",
      "Validation Accuracy: 92.50%\n",
      "Epoch 6/100, Training Loss: 0.0021, Validation Loss: 0.0510\n",
      "Validation Accuracy: 92.65%\n",
      "Epoch 7/100, Training Loss: 0.0017, Validation Loss: 0.0527\n",
      "Validation Accuracy: 93.78%\n",
      "Epoch 8/100, Training Loss: 0.0010, Validation Loss: 0.0445\n",
      "Validation Accuracy: 94.20%\n",
      "Epoch 9/100, Training Loss: 0.0008, Validation Loss: 0.0419\n",
      "Validation Accuracy: 94.77%\n",
      "Epoch 10/100, Training Loss: 0.0007, Validation Loss: 0.0390\n",
      "Validation Accuracy: 95.03%\n",
      "Epoch 11/100, Training Loss: 0.0006, Validation Loss: 0.0363\n",
      "Validation Accuracy: 94.55%\n",
      "Epoch 12/100, Training Loss: 0.0003, Validation Loss: 0.0403\n",
      "Validation Accuracy: 94.80%\n",
      "Epoch 13/100, Training Loss: 0.0003, Validation Loss: 0.0407\n",
      "Validation Accuracy: 94.61%\n",
      "Epoch 14/100, Training Loss: 0.0002, Validation Loss: 0.0429\n",
      "Validation Accuracy: 91.99%\n",
      "Epoch 15/100, Training Loss: 0.0007, Validation Loss: 0.0645\n",
      "Validation Accuracy: 93.28%\n",
      "Epoch 16/100, Training Loss: 0.0010, Validation Loss: 0.0527\n",
      "Validation Accuracy: 94.91%\n",
      "Epoch 17/100, Training Loss: 0.0008, Validation Loss: 0.0407\n",
      "Validation Accuracy: 92.55%\n",
      "Epoch 18/100, Training Loss: 0.0010, Validation Loss: 0.0592\n",
      "Validation Accuracy: 92.73%\n",
      "Epoch 19/100, Training Loss: 0.0009, Validation Loss: 0.0588\n",
      "Validation Accuracy: 94.57%\n",
      "Epoch 20/100, Training Loss: 0.0009, Validation Loss: 0.0431\n",
      "Validation Accuracy: 93.20%\n",
      "Epoch 21/100, Training Loss: 0.0006, Validation Loss: 0.0571\n",
      "Validation Accuracy: 93.40%\n",
      "Epoch 22/100, Training Loss: 0.0006, Validation Loss: 0.0543\n",
      "Validation Accuracy: 95.72%\n",
      "Epoch 23/100, Training Loss: 0.0004, Validation Loss: 0.0347\n",
      "Validation Accuracy: 93.77%\n",
      "Epoch 24/100, Training Loss: 0.0003, Validation Loss: 0.0551\n",
      "Validation Accuracy: 94.48%\n",
      "Epoch 25/100, Training Loss: 0.0003, Validation Loss: 0.0494\n",
      "Validation Accuracy: 95.36%\n",
      "Epoch 26/100, Training Loss: 0.0001, Validation Loss: 0.0422\n",
      "Validation Accuracy: 95.53%\n",
      "Epoch 27/100, Training Loss: 0.0001, Validation Loss: 0.0395\n",
      "Validation Accuracy: 95.49%\n",
      "Epoch 28/100, Training Loss: 0.0001, Validation Loss: 0.0425\n",
      "Validation Accuracy: 95.88%\n",
      "Epoch 29/100, Training Loss: 0.0000, Validation Loss: 0.0381\n",
      "Validation Accuracy: 95.98%\n",
      "Epoch 30/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 95.99%\n",
      "Epoch 31/100, Training Loss: 0.0000, Validation Loss: 0.0363\n",
      "Validation Accuracy: 96.05%\n",
      "Epoch 32/100, Training Loss: 0.0000, Validation Loss: 0.0363\n",
      "Validation Accuracy: 96.17%\n",
      "Epoch 33/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 96.00%\n",
      "Epoch 34/100, Training Loss: 0.0000, Validation Loss: 0.0364\n",
      "Validation Accuracy: 96.08%\n",
      "Epoch 35/100, Training Loss: 0.0000, Validation Loss: 0.0368\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 36/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.28%\n",
      "Epoch 37/100, Training Loss: 0.0000, Validation Loss: 0.0352\n",
      "Validation Accuracy: 96.38%\n",
      "Epoch 38/100, Training Loss: 0.0000, Validation Loss: 0.0346\n",
      "Validation Accuracy: 96.29%\n",
      "Epoch 39/100, Training Loss: 0.0000, Validation Loss: 0.0352\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 40/100, Training Loss: 0.0000, Validation Loss: 0.0342\n",
      "Validation Accuracy: 96.24%\n",
      "Epoch 41/100, Training Loss: 0.0000, Validation Loss: 0.0348\n",
      "Validation Accuracy: 96.21%\n",
      "Epoch 42/100, Training Loss: 0.0000, Validation Loss: 0.0355\n",
      "Validation Accuracy: 96.20%\n",
      "Epoch 43/100, Training Loss: 0.0000, Validation Loss: 0.0357\n",
      "Validation Accuracy: 96.22%\n",
      "Epoch 44/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.12%\n",
      "Epoch 45/100, Training Loss: 0.0000, Validation Loss: 0.0357\n",
      "Validation Accuracy: 96.30%\n",
      "Epoch 46/100, Training Loss: 0.0000, Validation Loss: 0.0365\n",
      "Validation Accuracy: 96.31%\n",
      "Epoch 47/100, Training Loss: 0.0000, Validation Loss: 0.0351\n",
      "Validation Accuracy: 96.21%\n",
      "Epoch 48/100, Training Loss: 0.0000, Validation Loss: 0.0357\n",
      "Validation Accuracy: 96.23%\n",
      "Epoch 49/100, Training Loss: 0.0000, Validation Loss: 0.0352\n",
      "Validation Accuracy: 96.28%\n",
      "Epoch 50/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 51/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 96.25%\n",
      "Epoch 52/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 96.20%\n",
      "Epoch 53/100, Training Loss: 0.0000, Validation Loss: 0.0357\n",
      "Validation Accuracy: 96.24%\n",
      "Epoch 54/100, Training Loss: 0.0000, Validation Loss: 0.0354\n",
      "Validation Accuracy: 96.53%\n",
      "Epoch 55/100, Training Loss: 0.0000, Validation Loss: 0.0343\n",
      "Validation Accuracy: 96.35%\n",
      "Epoch 56/100, Training Loss: 0.0000, Validation Loss: 0.0353\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 57/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 58/100, Training Loss: 0.0000, Validation Loss: 0.0352\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 59/100, Training Loss: 0.0000, Validation Loss: 0.0356\n",
      "Validation Accuracy: 96.22%\n",
      "Epoch 60/100, Training Loss: 0.0000, Validation Loss: 0.0356\n",
      "Validation Accuracy: 96.30%\n",
      "Epoch 61/100, Training Loss: 0.0000, Validation Loss: 0.0351\n",
      "Validation Accuracy: 96.32%\n",
      "Epoch 62/100, Training Loss: 0.0000, Validation Loss: 0.0360\n",
      "Validation Accuracy: 96.31%\n",
      "Epoch 63/100, Training Loss: 0.0000, Validation Loss: 0.0355\n",
      "Validation Accuracy: 96.20%\n",
      "Epoch 64/100, Training Loss: 0.0000, Validation Loss: 0.0364\n",
      "Validation Accuracy: 96.27%\n",
      "Epoch 65/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.28%\n",
      "Epoch 66/100, Training Loss: 0.0000, Validation Loss: 0.0360\n",
      "Validation Accuracy: 96.29%\n",
      "Epoch 67/100, Training Loss: 0.0000, Validation Loss: 0.0361\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 68/100, Training Loss: 0.0000, Validation Loss: 0.0351\n",
      "Validation Accuracy: 96.20%\n",
      "Epoch 69/100, Training Loss: 0.0000, Validation Loss: 0.0363\n",
      "Validation Accuracy: 96.31%\n",
      "Epoch 70/100, Training Loss: 0.0000, Validation Loss: 0.0356\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 71/100, Training Loss: 0.0000, Validation Loss: 0.0355\n",
      "Validation Accuracy: 96.28%\n",
      "Epoch 72/100, Training Loss: 0.0000, Validation Loss: 0.0362\n",
      "Validation Accuracy: 96.43%\n",
      "Epoch 73/100, Training Loss: 0.0000, Validation Loss: 0.0351\n",
      "Validation Accuracy: 96.35%\n",
      "Epoch 74/100, Training Loss: 0.0000, Validation Loss: 0.0354\n",
      "Validation Accuracy: 96.40%\n",
      "Epoch 75/100, Training Loss: 0.0000, Validation Loss: 0.0356\n",
      "Validation Accuracy: 96.28%\n",
      "Epoch 76/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.29%\n",
      "Epoch 77/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.38%\n",
      "Epoch 78/100, Training Loss: 0.0000, Validation Loss: 0.0366\n",
      "Validation Accuracy: 96.34%\n",
      "Epoch 79/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 96.39%\n",
      "Epoch 80/100, Training Loss: 0.0000, Validation Loss: 0.0357\n",
      "Validation Accuracy: 96.53%\n",
      "Epoch 81/100, Training Loss: 0.0000, Validation Loss: 0.0351\n",
      "Validation Accuracy: 96.22%\n",
      "Epoch 82/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 83/100, Training Loss: 0.0000, Validation Loss: 0.0361\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 84/100, Training Loss: 0.0000, Validation Loss: 0.0362\n",
      "Validation Accuracy: 96.52%\n",
      "Epoch 85/100, Training Loss: 0.0000, Validation Loss: 0.0348\n",
      "Validation Accuracy: 96.44%\n",
      "Epoch 86/100, Training Loss: 0.0000, Validation Loss: 0.0354\n",
      "Validation Accuracy: 96.35%\n",
      "Epoch 87/100, Training Loss: 0.0000, Validation Loss: 0.0367\n",
      "Validation Accuracy: 96.36%\n",
      "Epoch 88/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.37%\n",
      "Epoch 89/100, Training Loss: 0.0000, Validation Loss: 0.0361\n",
      "Validation Accuracy: 96.24%\n",
      "Epoch 90/100, Training Loss: 0.0000, Validation Loss: 0.0360\n",
      "Validation Accuracy: 96.23%\n",
      "Epoch 91/100, Training Loss: 0.0000, Validation Loss: 0.0366\n",
      "Validation Accuracy: 96.39%\n",
      "Epoch 92/100, Training Loss: 0.0000, Validation Loss: 0.0359\n",
      "Validation Accuracy: 96.33%\n",
      "Epoch 93/100, Training Loss: 0.0000, Validation Loss: 0.0361\n",
      "Validation Accuracy: 96.38%\n",
      "Epoch 94/100, Training Loss: 0.0000, Validation Loss: 0.0364\n",
      "Validation Accuracy: 96.36%\n",
      "Epoch 95/100, Training Loss: 0.0000, Validation Loss: 0.0358\n",
      "Validation Accuracy: 96.48%\n",
      "Epoch 96/100, Training Loss: 0.0000, Validation Loss: 0.0361\n",
      "Validation Accuracy: 96.31%\n",
      "Epoch 97/100, Training Loss: 0.0000, Validation Loss: 0.0355\n",
      "Validation Accuracy: 96.37%\n",
      "Epoch 98/100, Training Loss: 0.0000, Validation Loss: 0.0360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xf6\\xb9\\x8e\\x88\\xea\\x91\\xa0G\\xa1=\\xd2\\xc2=\\xf3\\xea?QW\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l\\x00m\\x00\\x84\\x00\\x85\\x00\\x86\\x00\\x87\\x00\\x88\\x00\\x89\\x00\\x96\\x00\\x97\\x00\\x98\\x00\\x99\\x00\\x9a\\x00\\x9b\\x00\\x9c\\x00\\x9d\\x00\\x9e\\x00\\x9f\\x00\\xa0\\x00\\xa1\\x00\\xa2\\x00\\xa3\\x00\\xa4\\x00\\xa5\\x00\\xa6\\x00\\xa7\\x00\\xba\\x00\\xbb\\x00\\xbc\\x00\\xbd\\x00\\xbe\\x00\\xbf\\x00\\xc0\\x00\\xc1\\x00', b\"\\xc3\\x00\\xc4\\x00\\xc5\\x13\\x01\\x13\\x02\\x13\\x03\\x13\\x04\\x13\\x05\\xc0\\x01\\xc0\\x02\\xc0\\x03\\xc0\\x04\\xc0\\x05\\xc0\\x06\\xc0\\x07\\xc0\\x08\\xc0\\t\\xc0\\n\\xc0\\x0b\\xc0\\x0c\\xc0\\r\\xc0\\x0e\\xc0\\x0f\\xc0\\x10\\xc0\\x11\\xc0\\x12\\xc0\\x13\\xc0\\x14\\xc0\\x15\\xc0\\x16\\xc0\\x17\\xc0\\x18\\xc0\\x19\\xc0#\\xc0$\\xc0%\\xc0&\\xc0'\\xc0(\\xc0)\\xc0*\\xc0+\\xc0,\\xc0-\\xc0.\\xc0/\\xc00\\xc01\\xc02\\xc0r\\xc0s\\xc0t\\xc0u\\xc0v\\xc0w\\xc0x\\xc0y\\xc0z\\xc0{\\xc0|\\xc0}\\xc0~\\xc0\\x7f\\xc0\\x80\\xc0\\x81\\xc0\\x82\\xc0\\x83\\xc0\\x84\\xc0\\x85\\xc0\\x86\\xc0\\x87\\xc0\\x88\\xc0\\x89\\xc0\\x8a\\xc0\\x8b\\xc0\\x8c\\xc0\\x8d\\xc0\\x8e\\xc0\\x8f\\xc0\\x90\\xc0\\x91\\xc0\\x92\\xc0\\x93\\xc0\\x94\\xc0\\x95\\xc0\\x96\\xc0\\x97\\xc0\\x98\\xc0\\x99\\xc0\\x9a\\xc0\\x9b\\xcc\\xa8\\xcc\\xa9\\xcc\\xaa\\xcc\\xab\\xcc\\xac\\xcc\\xad\"]\n",
      "Bad pipe message: %s [b'\\xf5:\\xb1\\xd8\\x18[a\\xdf\\xc6\\x85J\\xa5\\xd3\\xf8\\xb9f\\xe6\\x17\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l']\n",
      "Bad pipe message: %s [b'\\x1d\\xf8\\x0e\\xa0T\\xd8\\x01\\x9b;\\xb5\\xc7rw:\\x8c\\xf0+^\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00', b'\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x00']\n",
      "Bad pipe message: %s [b':\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l\\x00m\\x00\\x84\\x00\\x85\\x00\\x86\\x00\\x87\\x00\\x88\\x00\\x89\\x00\\x96\\x00\\x97\\x00']\n",
      "Bad pipe message: %s [b\"\\x99\\x00\\x9a\\x00\\x9b\\x00\\x9c\\x00\\x9d\\x00\\x9e\\x00\\x9f\\x00\\xa0\\x00\\xa1\\x00\\xa2\\x00\\xa3\\x00\\xa4\\x00\\xa5\\x00\\xa6\\x00\\xa7\\x00\\xba\\x00\\xbb\\x00\\xbc\\x00\\xbd\\x00\\xbe\\x00\\xbf\\x00\\xc0\\x00\\xc1\\x00\\xc2\\x00\\xc3\\x00\\xc4\\x00\\xc5\\x13\\x01\\x13\\x02\\x13\\x03\\x13\\x04\\x13\\x05\\xc0\\x01\\xc0\\x02\\xc0\\x03\\xc0\\x04\\xc0\\x05\\xc0\\x06\\xc0\\x07\\xc0\\x08\\xc0\\t\\xc0\\n\\xc0\\x0b\\xc0\\x0c\\xc0\\r\\xc0\\x0e\\xc0\\x0f\\xc0\\x10\\xc0\\x11\\xc0\\x12\\xc0\\x13\\xc0\\x14\\xc0\\x15\\xc0\\x16\\xc0\\x17\\xc0\\x18\\xc0\\x19\\xc0#\\xc0$\\xc0%\\xc0&\\xc0'\\xc0(\\xc0)\\xc0*\\xc0+\\xc0,\\xc0-\\xc0.\\xc0/\\xc00\\xc01\\xc02\\xc0r\\xc0s\\xc0t\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.24%\n",
      "Epoch 99/100, Training Loss: 0.0000, Validation Loss: 0.0364\n",
      "Validation Accuracy: 96.51%\n",
      "Epoch 100/100, Training Loss: 0.0000, Validation Loss: 0.0355\n"
     ]
    }
   ],
   "source": [
    "modelf = train_full(labeled_loader, val_loader,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMEWF_2YvsI1",
    "outputId": "685d68ab-22f2-465f-b07b-f625c6df8663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/6c6r69vs6xb7_882kknxt6wh0000gs/T/ipykernel_59583/722626161.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ch = torch.load(\"content/Labeled1/Newfulldatamodel.pt\", device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.99%\n",
      "Epoch 1/100, Training Loss: 0.2062, Validation Loss: 0.1422\n",
      "Validation Accuracy: 94.51%\n",
      "Epoch 2/100, Training Loss: 0.1438, Validation Loss: 0.1478\n",
      "Validation Accuracy: 96.10%\n",
      "Epoch 3/100, Training Loss: 0.1319, Validation Loss: 0.1094\n",
      "Validation Accuracy: 96.02%\n",
      "Epoch 4/100, Training Loss: 0.1305, Validation Loss: 0.1119\n",
      "Validation Accuracy: 96.20%\n",
      "Epoch 5/100, Training Loss: 0.1241, Validation Loss: 0.1070\n",
      "Validation Accuracy: 96.20%\n",
      "Epoch 6/100, Training Loss: 0.1231, Validation Loss: 0.1104\n",
      "Validation Accuracy: 96.68%\n",
      "Epoch 7/100, Training Loss: 0.1187, Validation Loss: 0.0964\n",
      "Validation Accuracy: 92.36%\n",
      "Epoch 8/100, Training Loss: 0.1122, Validation Loss: 0.1873\n",
      "Validation Accuracy: 96.67%\n",
      "Epoch 9/100, Training Loss: 0.1120, Validation Loss: 0.0942\n",
      "Validation Accuracy: 96.14%\n",
      "Epoch 10/100, Training Loss: 0.1076, Validation Loss: 0.1149\n",
      "Validation Accuracy: 96.81%\n",
      "Epoch 11/100, Training Loss: 0.1066, Validation Loss: 0.0914\n",
      "Validation Accuracy: 96.75%\n",
      "Epoch 12/100, Training Loss: 0.1084, Validation Loss: 0.0955\n",
      "Validation Accuracy: 96.66%\n",
      "Epoch 13/100, Training Loss: 0.1065, Validation Loss: 0.0988\n",
      "Validation Accuracy: 96.91%\n",
      "Epoch 14/100, Training Loss: 0.1010, Validation Loss: 0.0891\n",
      "Validation Accuracy: 96.80%\n",
      "Epoch 15/100, Training Loss: 0.1105, Validation Loss: 0.0941\n",
      "Validation Accuracy: 96.60%\n",
      "Epoch 16/100, Training Loss: 0.1057, Validation Loss: 0.1004\n",
      "Validation Accuracy: 95.21%\n",
      "Epoch 17/100, Training Loss: 0.1065, Validation Loss: 0.1550\n",
      "Validation Accuracy: 96.85%\n",
      "Epoch 18/100, Training Loss: 0.1075, Validation Loss: 0.0888\n",
      "Validation Accuracy: 96.78%\n",
      "Epoch 19/100, Training Loss: 0.1023, Validation Loss: 0.0892\n",
      "Validation Accuracy: 96.88%\n",
      "Epoch 20/100, Training Loss: 0.1080, Validation Loss: 0.0876\n",
      "Validation Accuracy: 96.95%\n",
      "Epoch 21/100, Training Loss: 0.1023, Validation Loss: 0.0898\n",
      "Validation Accuracy: 97.08%\n",
      "Epoch 22/100, Training Loss: 0.1052, Validation Loss: 0.0839\n",
      "Validation Accuracy: 96.97%\n",
      "Epoch 23/100, Training Loss: 0.1062, Validation Loss: 0.0852\n",
      "Validation Accuracy: 96.95%\n",
      "Epoch 24/100, Training Loss: 0.0995, Validation Loss: 0.0852\n",
      "Validation Accuracy: 93.00%\n",
      "Epoch 25/100, Training Loss: 0.1027, Validation Loss: 0.1755\n",
      "Validation Accuracy: 96.96%\n",
      "Epoch 26/100, Training Loss: 0.1049, Validation Loss: 0.0899\n",
      "Validation Accuracy: 94.83%\n",
      "Epoch 27/100, Training Loss: 0.1064, Validation Loss: 0.1310\n",
      "Validation Accuracy: 97.16%\n",
      "Epoch 28/100, Training Loss: 0.1023, Validation Loss: 0.0833\n",
      "Validation Accuracy: 97.14%\n",
      "Epoch 29/100, Training Loss: 0.1017, Validation Loss: 0.0825\n",
      "Validation Accuracy: 96.12%\n",
      "Epoch 30/100, Training Loss: 0.0995, Validation Loss: 0.1221\n",
      "Validation Accuracy: 97.06%\n",
      "Epoch 31/100, Training Loss: 0.1080, Validation Loss: 0.0900\n",
      "Validation Accuracy: 97.21%\n",
      "Epoch 32/100, Training Loss: 0.1003, Validation Loss: 0.0809\n",
      "Validation Accuracy: 96.34%\n",
      "Epoch 33/100, Training Loss: 0.0999, Validation Loss: 0.0987\n",
      "Validation Accuracy: 95.80%\n",
      "Epoch 34/100, Training Loss: 0.1058, Validation Loss: 0.1125\n",
      "Validation Accuracy: 97.19%\n",
      "Epoch 35/100, Training Loss: 0.0980, Validation Loss: 0.0824\n",
      "Validation Accuracy: 97.03%\n",
      "Epoch 36/100, Training Loss: 0.1039, Validation Loss: 0.0835\n",
      "Validation Accuracy: 97.30%\n",
      "Epoch 37/100, Training Loss: 0.1016, Validation Loss: 0.0813\n",
      "Validation Accuracy: 97.20%\n",
      "Epoch 38/100, Training Loss: 0.1023, Validation Loss: 0.0821\n",
      "Validation Accuracy: 96.96%\n",
      "Epoch 39/100, Training Loss: 0.1003, Validation Loss: 0.0925\n",
      "Validation Accuracy: 96.22%\n",
      "Epoch 40/100, Training Loss: 0.0993, Validation Loss: 0.1198\n",
      "Validation Accuracy: 97.32%\n",
      "Epoch 41/100, Training Loss: 0.1015, Validation Loss: 0.0805\n",
      "Validation Accuracy: 96.77%\n",
      "Epoch 42/100, Training Loss: 0.0962, Validation Loss: 0.1013\n",
      "Validation Accuracy: 96.78%\n",
      "Epoch 43/100, Training Loss: 0.0971, Validation Loss: 0.0988\n",
      "Validation Accuracy: 96.75%\n",
      "Epoch 44/100, Training Loss: 0.0975, Validation Loss: 0.0988\n",
      "Validation Accuracy: 97.03%\n",
      "Epoch 45/100, Training Loss: 0.0975, Validation Loss: 0.0833\n",
      "Validation Accuracy: 95.86%\n",
      "Epoch 46/100, Training Loss: 0.0947, Validation Loss: 0.1375\n",
      "Validation Accuracy: 97.27%\n",
      "Epoch 47/100, Training Loss: 0.1016, Validation Loss: 0.0800\n",
      "Validation Accuracy: 97.09%\n",
      "Epoch 48/100, Training Loss: 0.1031, Validation Loss: 0.0891\n",
      "Validation Accuracy: 96.65%\n",
      "Epoch 49/100, Training Loss: 0.0996, Validation Loss: 0.0898\n",
      "Validation Accuracy: 97.26%\n",
      "Epoch 50/100, Training Loss: 0.1018, Validation Loss: 0.0808\n",
      "Validation Accuracy: 96.96%\n",
      "Epoch 51/100, Training Loss: 0.0954, Validation Loss: 0.0849\n",
      "Validation Accuracy: 96.58%\n",
      "Epoch 52/100, Training Loss: 0.0935, Validation Loss: 0.0935\n",
      "Validation Accuracy: 97.05%\n",
      "Epoch 53/100, Training Loss: 0.0980, Validation Loss: 0.0908\n",
      "Validation Accuracy: 97.20%\n",
      "Epoch 54/100, Training Loss: 0.0948, Validation Loss: 0.0817\n",
      "Validation Accuracy: 96.02%\n",
      "Epoch 55/100, Training Loss: 0.0952, Validation Loss: 0.1290\n",
      "Validation Accuracy: 97.03%\n",
      "Epoch 56/100, Training Loss: 0.0969, Validation Loss: 0.0903\n",
      "Validation Accuracy: 96.13%\n",
      "Epoch 57/100, Training Loss: 0.0911, Validation Loss: 0.1031\n",
      "Validation Accuracy: 97.29%\n",
      "Epoch 58/100, Training Loss: 0.0983, Validation Loss: 0.0849\n",
      "Validation Accuracy: 95.78%\n",
      "Epoch 59/100, Training Loss: 0.0941, Validation Loss: 0.1118\n",
      "Validation Accuracy: 95.83%\n",
      "Epoch 60/100, Training Loss: 0.0961, Validation Loss: 0.1378\n",
      "Validation Accuracy: 97.16%\n",
      "Epoch 61/100, Training Loss: 0.0955, Validation Loss: 0.0815\n",
      "Validation Accuracy: 97.30%\n",
      "Epoch 62/100, Training Loss: 0.0964, Validation Loss: 0.0791\n",
      "Validation Accuracy: 96.25%\n",
      "Epoch 63/100, Training Loss: 0.0952, Validation Loss: 0.1000\n",
      "Validation Accuracy: 97.24%\n",
      "Epoch 64/100, Training Loss: 0.0952, Validation Loss: 0.0800\n",
      "Validation Accuracy: 97.23%\n",
      "Epoch 65/100, Training Loss: 0.0982, Validation Loss: 0.0795\n",
      "Validation Accuracy: 96.43%\n",
      "Epoch 66/100, Training Loss: 0.0917, Validation Loss: 0.0956\n",
      "Validation Accuracy: 96.01%\n",
      "Epoch 67/100, Training Loss: 0.0932, Validation Loss: 0.1283\n",
      "Validation Accuracy: 97.44%\n",
      "Epoch 68/100, Training Loss: 0.0979, Validation Loss: 0.0767\n",
      "Validation Accuracy: 97.40%\n",
      "Epoch 69/100, Training Loss: 0.0935, Validation Loss: 0.0792\n",
      "Validation Accuracy: 97.39%\n",
      "Epoch 70/100, Training Loss: 0.0977, Validation Loss: 0.0779\n",
      "Validation Accuracy: 97.14%\n",
      "Epoch 71/100, Training Loss: 0.0978, Validation Loss: 0.0876\n",
      "Validation Accuracy: 96.79%\n",
      "Epoch 72/100, Training Loss: 0.0921, Validation Loss: 0.0872\n",
      "Validation Accuracy: 97.35%\n",
      "Epoch 73/100, Training Loss: 0.0963, Validation Loss: 0.0786\n",
      "Validation Accuracy: 97.14%\n",
      "Epoch 74/100, Training Loss: 0.0952, Validation Loss: 0.0809\n",
      "Validation Accuracy: 97.35%\n",
      "Epoch 75/100, Training Loss: 0.0964, Validation Loss: 0.0796\n",
      "Validation Accuracy: 96.24%\n",
      "Epoch 76/100, Training Loss: 0.0976, Validation Loss: 0.0998\n",
      "Validation Accuracy: 97.27%\n",
      "Epoch 77/100, Training Loss: 0.0951, Validation Loss: 0.0848\n",
      "Validation Accuracy: 97.33%\n",
      "Epoch 78/100, Training Loss: 0.0976, Validation Loss: 0.0771\n",
      "Validation Accuracy: 97.28%\n",
      "Epoch 79/100, Training Loss: 0.0967, Validation Loss: 0.0778\n",
      "Validation Accuracy: 97.09%\n",
      "Epoch 80/100, Training Loss: 0.0935, Validation Loss: 0.0882\n",
      "Validation Accuracy: 97.38%\n",
      "Epoch 81/100, Training Loss: 0.0935, Validation Loss: 0.0780\n",
      "Validation Accuracy: 96.59%\n",
      "Epoch 82/100, Training Loss: 0.0985, Validation Loss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x18\\xa0)<\\x87=\\xdf\\xba\\xfa\\xb4\\x9aF\\x05H\\xb0o7\\xd0\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C']\n",
      "Bad pipe message: %s [b'\\xa7\\x98\\x1a1\\xf3\\xa3\\x86\\x83,\\xc9\\xf9\\x884b\\x94\\x04\\xd9\\x0f\\x00\\x01|\\x00\\x00\\x00\\x01\\x00']\n",
      "Bad pipe message: %s [b'\\x89\\xbbm\"R`\\x94\\xb5\\xb3\\x16Z\\xb6\\xc4:<\\x92g\\x00\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00']\n",
      "Bad pipe message: %s [b'/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00']\n",
      "Bad pipe message: %s [b'\\x03']\n",
      "Bad pipe message: %s [b'=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l\\x00m\\x00\\x84\\x00\\x85\\x00\\x86\\x00\\x87\\x00\\x88\\x00\\x89\\x00\\x96\\x00\\x97\\x00\\x98\\x00\\x99\\x00\\x9a\\x00\\x9b\\x00\\x9c']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.89%\n",
      "Epoch 83/100, Training Loss: 0.0948, Validation Loss: 0.0863\n",
      "Validation Accuracy: 95.84%\n",
      "Epoch 84/100, Training Loss: 0.0977, Validation Loss: 0.1378\n",
      "Validation Accuracy: 97.08%\n",
      "Epoch 85/100, Training Loss: 0.0957, Validation Loss: 0.0801\n",
      "Validation Accuracy: 97.35%\n",
      "Epoch 86/100, Training Loss: 0.0971, Validation Loss: 0.0763\n",
      "Validation Accuracy: 97.37%\n",
      "Epoch 87/100, Training Loss: 0.0934, Validation Loss: 0.0773\n",
      "Validation Accuracy: 97.32%\n",
      "Epoch 88/100, Training Loss: 0.0979, Validation Loss: 0.0854\n",
      "Validation Accuracy: 97.08%\n",
      "Epoch 89/100, Training Loss: 0.0963, Validation Loss: 0.0818\n",
      "Validation Accuracy: 97.26%\n",
      "Epoch 90/100, Training Loss: 0.0943, Validation Loss: 0.0785\n",
      "Validation Accuracy: 97.33%\n",
      "Epoch 91/100, Training Loss: 0.0947, Validation Loss: 0.0785\n",
      "Validation Accuracy: 93.90%\n",
      "Epoch 92/100, Training Loss: 0.0913, Validation Loss: 0.2307\n",
      "Validation Accuracy: 97.24%\n",
      "Epoch 93/100, Training Loss: 0.0971, Validation Loss: 0.0849\n",
      "Validation Accuracy: 97.05%\n",
      "Epoch 94/100, Training Loss: 0.0934, Validation Loss: 0.0937\n",
      "Validation Accuracy: 96.69%\n",
      "Epoch 95/100, Training Loss: 0.0949, Validation Loss: 0.0888\n",
      "Validation Accuracy: 96.45%\n",
      "Epoch 96/100, Training Loss: 0.0962, Validation Loss: 0.0940\n",
      "Validation Accuracy: 97.27%\n",
      "Epoch 97/100, Training Loss: 0.0950, Validation Loss: 0.0787\n",
      "Validation Accuracy: 97.13%\n",
      "Epoch 98/100, Training Loss: 0.0942, Validation Loss: 0.0881\n",
      "Validation Accuracy: 97.34%\n",
      "Epoch 99/100, Training Loss: 0.0976, Validation Loss: 0.0790\n",
      "Validation Accuracy: 97.33%\n",
      "Epoch 100/100, Training Loss: 0.0976, Validation Loss: 0.0769\n",
      "Classifier Completed: 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(9,10):\n",
    "  modelb = train_binary_call(i,labeled_loader, val_loader,100)\n",
    "  print(\"Classifier Completed: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "mASeUY_KsdZg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/6c6r69vs6xb7_882kknxt6wh0000gs/T/ipykernel_59583/259134756.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chh = torch.load(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.87%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.87"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"content/Labeled1/Newfulldatamodel.pt\"\n",
    "chh = torch.load(PATH)\n",
    "modelq = ResNet18().to(device)\n",
    "modelq.load_state_dict(chh[\"model_state_dict\"])\n",
    "modelq.eval()\n",
    "a = test(modelq, test_loader)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "w20OFaCCezMN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loader1 512\n",
      "initial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/6c6r69vs6xb7_882kknxt6wh0000gs/T/ipykernel_10214/4064888636.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ch = torch.load(\"content/Labeled1/Newbinarydatamodel9\" + \".pt\",device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After For\n",
      "Creating dataset\n",
      "loader:  60000\n",
      "Indices:  48893\n",
      "Remaining unlabeled data after model : 0 11107\n",
      "Training after0 1702\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "loader:  11107\n",
      "Indices:  9074\n",
      "Remaining unlabeled data after model : 1 2033\n",
      "Training after1 1080\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "loader:  2033\n",
      "Indices:  1666\n",
      "Remaining unlabeled data after model : 2 367\n",
      "Training after2 964\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "loader:  367\n",
      "Indices:  286\n",
      "Remaining unlabeled data after model : 3 81\n",
      "Training after3 942\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "loader:  81\n",
      "Indices:  63\n",
      "Remaining unlabeled data after model : 4 18\n",
      "Training after4 939\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "loader:  18\n",
      "Indices:  13\n",
      "Remaining unlabeled data after model : 5 5\n",
      "Training after5 938\n"
     ]
    }
   ],
   "source": [
    "allacc = []\n",
    "test_loader1 = unlabeled_loader\n",
    "print(\"Initial loader1\", len(test_loader1)*64)\n",
    "for i in range(6):\n",
    "  ch = torch.load(\"content/Labeled1/Newbinarydatamodel9\" + \".pt\",device)\n",
    "  model1 = ResNet18(num_classes=2).to(device)\n",
    "  model1.load_state_dict(ch[\"model_state_dict\"])\n",
    "  model1.eval()\n",
    "  dlt = getbinaryloader(test_loader1,i)\n",
    "  lt,ul = predict_pseudolabels(dlt, model1)\n",
    "  lbl_tr, unlbld = getdataloaders(train_loader.dataset, ul, lt, i)\n",
    "  test_loader1 = unlbld\n",
    "  print(\"Remaining unlabeled data after model : \" + str(i), len(ul))\n",
    "  print(\"Training after\" + str(i), len(lbl_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3P6JIqPqM_d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/6c6r69vs6xb7_882kknxt6wh0000gs/T/ipykernel_10214/3611112832.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ch = torch.load(\"content/Labeled1/Newbinarydatamodel9\" + \".pt\",device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'val_accuracy', 'outputs', 'class_num', 'seed'])\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 0\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 1\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 2\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 3\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 4\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 5\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 6\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 7\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 8\n",
      "Model: ResNet18(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") Validation Accuracy: 97.33166666666666 classifier: 9\n",
      "Initial loader1 512\n",
      "initial\n",
      "After For\n",
      "Creating dataset\n",
      "loader:  60000\n"
     ]
    }
   ],
   "source": [
    "allacc = []\n",
    "test_loader1 = unlabeled_loader\n",
    "models = sortmodels()\n",
    "print(\"Initial loader1\", len(test_loader1)*64)\n",
    "i=0\n",
    "for i in range(6):\n",
    "\n",
    "  dlt = getbinaryloader(test_loader1,i)\n",
    "  lbtotrain,ulleft = predict_pseudolabels(dlt,models[i]['model'])\n",
    "  lbl_tr, unlbld = getdataloaders(labeled_loader.dataset, ulleft, lbtotrain,i)\n",
    "  test_loader1 = unlbld\n",
    "  print(\"Remaining unlabeled data after model : \" + str(i), len(ul))\n",
    "  print(\"Training after\" + str(i), len(lbl_tr))\n",
    "\n",
    "  torch.save({\n",
    "      'unlabeled': len(ulleft),\n",
    "      'labeled': len(lbl_tr)\n",
    "  }, \"content/Labeled1/num_samples_c\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vByPm-ssaQk1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "env-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
